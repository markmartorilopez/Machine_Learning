{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "- GOALS:\n",
    "- - Movies most likely to be rated 5 stars by user.\n",
    "- - Products most likely to be purchased.\n",
    "- - Ads most likely to be clicked on + higher bid.\n",
    "- - Products generating the largest profit by the company. Cheapily produced etc.\n",
    "- - Video leading to maximum watch time.\n",
    "-\n",
    "-\n",
    "\n",
    "## 1. Collaborative Filtering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PER-ITEM FEATURES: When we have the following information:\n",
    "- i -> Movie(i)\n",
    "- j -> Person(j)\n",
    "- x(i) -> Feature vector for Movie(i) -> Romance and Action values of Movie(i)\n",
    "- w(j), b(j) -> Parameters to be guessed for user(j).\n",
    "- y(i,j) -> Rating evaluation given by user(j) on Movie(i)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](../../figures/ReccSystems.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COST FUNCTION:\n",
    "##### Where:\n",
    "- - m(j) -> Number of movies rated by USer(j).\n",
    "- - r(i,j) = 1 if User(j) has rated Movie(i), else 0.\n",
    "- - We want to learn w(j) and b(j).\n",
    "- - \\+ Regularization parameter lambda."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text2](../../figures/cf.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we added a new column \"Eve\" with all \"?\".\n",
    "- Since it has not reviewed any movie yet, the parameters w(j=5) would be [0 0] once they are minimized. Since the user has no effect on the existant ratings.\n",
    "- Then, the predictions of all movies for this user would be ZERO.\n",
    "\n",
    "To solve this:\n",
    "- - Mean Normalization:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text02](../../figures/mn.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Take all of these ratings and for each movie, compute the average rating that was given.\n",
    "- This way, we are normalizing the movies ratings. This is a value that we will store per each movie.\n",
    "- This value, will be later on added in the Cost Function, in order to avoid negative ratings,\n",
    "- as well as to predict an output with more common sense:\n",
    "- - We can't say that the user will not like the movie (y = 0).\n",
    "- - But we can say, the average rating of this movie is y = average(i), and it is probable that the new user falls into the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPLEMENTATION\n",
    "\n",
    "# - TensorFlow : Allows to use Gradient Descent without manually calculating the derivatives of the Cost Function.\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_function(X, W, b, Ynorm, R):\n",
    "    return W * X + b\n",
    "\n",
    "def loss_function():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's guess the parameter w(j) of j=5. \n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate = 1e-1)\n",
    "\n",
    "iterations = 200\n",
    "\n",
    "# AUTOMATIC DIFFERENTATION\n",
    "for iter in range(iterations):\n",
    "\n",
    "    # TensorFlow gradient Tape -> Automatic Differentation\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        predicted_rating = forward_function(X, W, b, Ynorm, R)\n",
    "        # Repeat until convergence:\n",
    "        # 1 - Get J w.r.t. w\n",
    "        # 2 - Get J w.r.t. b\n",
    "        # 3 - Update x = x - alpha * J(w,b)\n",
    "        loss_value = loss_function(num_users, num_movies, lambda_regularization)\n",
    "    # Calculate the Gradient of Loss Function to upgrade w value.\n",
    "    gradients_loss = tape.gradient(loss_value, [X, W, b])  # Get gradient of loss function w.r.t. w.\n",
    "\n",
    "    # Upgrade w parameter to minimize the loss:\n",
    "    optimizer.apply_gradients( zip(gradients_loss, [X, W, b]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA ADQUISITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Label: Liked the Movie? Finished the movie?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict the probability of y(i,j) = 1, given by:\n",
    "- - g(w(j) · x(i) \\+ b(j))\n",
    "\n",
    "Wehere g(Z):\n",
    "- - 1 / ( 1 \\+ e^-z )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text3](../b.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COST FUNCTION:\n",
    "- - Binary Cross Entropy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text1](../bb.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From (i, j) to r(i, j) = 1. So the BCEntropy of all the Movies that have a target value (1 or 0)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Content-based filtering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In collaborative filtering, we had number of users give ratings of different items. In contrast, in content-based filtering, we have features of users and features of items and we want to find a way to find good matches between the users and the items. The way we're going to do so is to compute these vectors, v_u for the users and v_m for the items over the movies, and then take dot products between them to try to find good matches."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning Approach:\n",
    "-  Given vector X_u (containing features of Users, such as Age, gender, demographics...), we have to create a new vector V_u:\n",
    "- - Containing, user preferences regarding the movies. Ex: How much do they like each gender:\n",
    "- - + Avg Rating Comedy\n",
    "- - + Avg Rating Action\n",
    "- - + Avg Rating Romance\n",
    "-\n",
    "- At the same time, given vector X_m (Containing year of the Movie, duration, stars etc...) we have to compute a vector V_m of same length as V_u, such that:\n",
    "- - It describes the movie content:\n",
    "- - + How much is j a Comedy movie.\n",
    "- - + How much is j an Action movie.\n",
    "- - + How much is j a Romance movie.\n",
    "-\n",
    "- The combination (dot product) of both, should be a good estimator of the rating that user j gives to movie i."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V_u : User network\n",
    "-\n",
    "- Input: X_u (Age, gender, Demographics, )\n",
    "- Layers:\n",
    "- - Input Layer Units -> len(X_u)\n",
    "- - Middle Layer Units -> len(prev_layer) / 2 \n",
    "- - Output Layer Units -> len(V_u) + Sigmoid function if Binary to predict the probability that y_ij = 1.\n",
    "-\n",
    "- Output: V_u -> Describes the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_network = tf.keras.models.Sequential(\n",
    "    [\n",
    "    tf.keras.layers.Dense(256,\n",
    "                          activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Dense(128,\n",
    "                          activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Dense(32)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V_m : Movie network\n",
    "-\n",
    "- Input: X_m (Year, actors, genere...)\n",
    "- Layers:\n",
    "- - Input Layer Units -> len(X_m)\n",
    "- - Middle Layer Units -> len(prev_layer) / 2 \n",
    "- - Output Layer Units -> len(V_m) + Sigmoid function if Binary to predict the probability that y_ij = 1.\n",
    "-\n",
    "- Output: V_m -> Describes the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_network = tf.keras.models.Sequential(\n",
    "    [\n",
    "    tf.keras.layers.Dense(256,\n",
    "                          activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Dense(128,\n",
    "                          activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Dense(32)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Last layer has 32 units -> 32 numbers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicted_Rating = V_u · V_m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function J:\n",
    "- Assuming that we have some data, of some users at least rating some movies...\n",
    "-\n",
    "- Sum over all pairs i and j, where we have labels [ r( i, j) = 1]:\n",
    "- - (Diff bw prediction - y_i,j )**2\n",
    "-\n",
    "- Then use Gradient Descent or another optimization algorithm, to tune the parameters of the network to cause the loss function to be as small as possible.\n",
    "- To regularize: We can add: NN regularization term to keep values of the parameters small and avoid exploiding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract the user features from the raw data, and feeds it to the user NN.\n",
    "input_user = tf.keras.layers.Input(\n",
    "    shape=(\n",
    "        num_user_features\n",
    "    )\n",
    ")\n",
    "vector_user = user_network(input_user)\n",
    "\n",
    "# NORMALIZE THE N2 NORM of the vector:\n",
    "# To improve the performance of this approach, we can normalize the length of the vector_user to be equal to 1.\n",
    "vector_user = tf.linalg.l2_normalize(vector_user, axis=1)\n",
    "\n",
    "# 1. Extract the item features from the raw data, and feeds it to the user NN.\n",
    "input_item = tf.keras.layers.Input(\n",
    "    shape=(\n",
    "        num_item_features\n",
    "    )\n",
    ")\n",
    "vector_item = item_network(input_user)\n",
    "\n",
    "# To improve the performance of this approach, we can normalize the length of the vector_user to be equal to 1.\n",
    "vector_item = tf.linalg.l2_normalize(vector_item, axis=1)\n",
    "\n",
    "# Measure the similarity of the 2 vectors into a prediction:\n",
    "# Output of the neural network:\n",
    "prediction = tf.keras.layers.Dot(axes=1) ([vector_user, vector_item])\n",
    "\n",
    "# Specify the inputs and output of the model\n",
    "model = Model([input_user, input_item], prediction)\n",
    "\n",
    "cost_fn = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use this information?\n",
    "### Find items similar to 1 item:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- V_u_j is a vector of length 32 that describes a user j that have features X_u_j. Similarly,\n",
    "- V_m_i is a vector of length 32 that describes a movie with these features X_m_i.\n",
    "-\n",
    "- Given a specific movie, we want to find movies that are similar to it:\n",
    "- - V_m_i describes movie i. So, look for other movies k, so that distance(V_m_i, V_m_k) is small. ||V_m_k - V_m_i||**2\n",
    "-\n",
    "- This can be precomputed. So that when user browse a movie, we already have searched the top 10 most similar ones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale up to large data set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When a new user signs in and watches a couple of movies, you would like to recommend him the next one.\n",
    "- However, we can not run every single time, 1M of data adding the new users, and compute the dot product to predict, every time.\n",
    "- It would be high-computationally inneficient. \n",
    "- SOLUTION?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Steps:\n",
    "-\n",
    "- 1. **Retrieval**:\n",
    "- - Generate large list of plausible item candidates. The more items, better performance (since you can recommend from a broader catalog of items) but slower recommendations.\n",
    "- - + For each of the last 10 movies watched by the user, find 10 most similar ones. [PRECOMPUTED]\n",
    "- - + For most viewed genres, find the top 10 movies.\n",
    "- - + Top 20 movies in the country.\n",
    "- - * Analyze the results:\n",
    "- - - * Carry out offline experiments to see if retrieving additional items results in more relevant recommendations (i.e. p(y_ij) = 1) of items displayed to user are higher.\n",
    "- - Combine retrieved items into list, removing duplicates and items already watched/purchased. With around 100s recommended movies.\n",
    "- - The goal is to ensure broad coverage.\n",
    "- \n",
    "- 2. **Ranking Step**:\n",
    "- - Take the list from the retrieval and rank it using the learned model.\n",
    "- - + For each movie-user vectors pairs, compute the predicted rating.\n",
    "- - + Based on the rating, you can display the ranked items.\n",
    "- - * To predict this rating, we don't need to run inference again, but we should have stored the V_u and V_m and now just dot product them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f98b681bdce3f32f2bb16a15c5969ee48ce98cc956bbc786f6e97b4d40fde33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
