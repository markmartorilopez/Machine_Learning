{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f12ec631-0d57-4a6d-a0ec-7940abac6968",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "- Entrying in Deep Learning.\n",
    "- Autoencoders are used in dimensionality reduction, image compression, image denoising, feature extraction, and anomaly detection.\n",
    "- Autoencoders: Neural Network that copies the input values to the output values. Since it does not require labels, it is considered Unsupervised.\n",
    "- - If the input is a set of vectors, we want to transform them doing certain operations into a new set of vectors containing the most important patterns of the inputs and ignoring noise, called: Feature Vectors. This new set of vectors, needs to be able to get transformed back into the same input vectors following other operations.\n",
    "* The aforementioned \"Feature Vectors\" is what we are interested on.\n",
    "* There should be less hidden neurons than input neurons, to avoid overfitting or learning useles information such as noise.\n",
    "* Quite useful when data problems are complex and non-linear."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8a6a403",
   "metadata": {},
   "source": [
    "### Prepare data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9021294f-5f48-4f53-8b36-773852531a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.396090</td>\n",
       "      <td>2.092611</td>\n",
       "      <td>2.073392</td>\n",
       "      <td>1.988262</td>\n",
       "      <td>1.953473</td>\n",
       "      <td>2.450997</td>\n",
       "      <td>1.631040</td>\n",
       "      <td>1.746182</td>\n",
       "      <td>1.898050</td>\n",
       "      <td>2.380148</td>\n",
       "      <td>...</td>\n",
       "      <td>1.703454</td>\n",
       "      <td>2.502966</td>\n",
       "      <td>2.119108</td>\n",
       "      <td>2.106098</td>\n",
       "      <td>2.165173</td>\n",
       "      <td>2.340826</td>\n",
       "      <td>2.170109</td>\n",
       "      <td>1.749139</td>\n",
       "      <td>1.678661</td>\n",
       "      <td>1.829647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.775596</td>\n",
       "      <td>1.829438</td>\n",
       "      <td>2.054768</td>\n",
       "      <td>1.577190</td>\n",
       "      <td>1.594549</td>\n",
       "      <td>1.373357</td>\n",
       "      <td>1.946647</td>\n",
       "      <td>1.841420</td>\n",
       "      <td>1.595761</td>\n",
       "      <td>2.538094</td>\n",
       "      <td>...</td>\n",
       "      <td>1.974274</td>\n",
       "      <td>1.621608</td>\n",
       "      <td>2.003085</td>\n",
       "      <td>2.076871</td>\n",
       "      <td>1.788868</td>\n",
       "      <td>2.062829</td>\n",
       "      <td>2.084499</td>\n",
       "      <td>2.267568</td>\n",
       "      <td>1.536939</td>\n",
       "      <td>2.132725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.835679</td>\n",
       "      <td>1.612100</td>\n",
       "      <td>2.174908</td>\n",
       "      <td>2.084460</td>\n",
       "      <td>2.472896</td>\n",
       "      <td>2.029110</td>\n",
       "      <td>2.410107</td>\n",
       "      <td>2.282164</td>\n",
       "      <td>2.208201</td>\n",
       "      <td>2.106240</td>\n",
       "      <td>...</td>\n",
       "      <td>2.035652</td>\n",
       "      <td>2.065291</td>\n",
       "      <td>2.197711</td>\n",
       "      <td>2.288806</td>\n",
       "      <td>2.480274</td>\n",
       "      <td>1.946207</td>\n",
       "      <td>1.947120</td>\n",
       "      <td>1.754344</td>\n",
       "      <td>2.265033</td>\n",
       "      <td>2.119050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.420241</td>\n",
       "      <td>2.158485</td>\n",
       "      <td>1.958602</td>\n",
       "      <td>1.903787</td>\n",
       "      <td>2.230522</td>\n",
       "      <td>1.984789</td>\n",
       "      <td>1.964441</td>\n",
       "      <td>2.360795</td>\n",
       "      <td>1.820773</td>\n",
       "      <td>2.116560</td>\n",
       "      <td>...</td>\n",
       "      <td>2.040977</td>\n",
       "      <td>1.511381</td>\n",
       "      <td>1.834332</td>\n",
       "      <td>2.070046</td>\n",
       "      <td>1.911699</td>\n",
       "      <td>1.816916</td>\n",
       "      <td>2.213950</td>\n",
       "      <td>2.099758</td>\n",
       "      <td>2.259999</td>\n",
       "      <td>2.039066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.646926</td>\n",
       "      <td>1.778067</td>\n",
       "      <td>1.760959</td>\n",
       "      <td>1.894087</td>\n",
       "      <td>1.888225</td>\n",
       "      <td>2.228021</td>\n",
       "      <td>2.489542</td>\n",
       "      <td>2.326377</td>\n",
       "      <td>1.969615</td>\n",
       "      <td>2.001316</td>\n",
       "      <td>...</td>\n",
       "      <td>2.063858</td>\n",
       "      <td>2.341009</td>\n",
       "      <td>1.844115</td>\n",
       "      <td>2.076399</td>\n",
       "      <td>1.742857</td>\n",
       "      <td>1.969530</td>\n",
       "      <td>1.821128</td>\n",
       "      <td>1.946249</td>\n",
       "      <td>1.678283</td>\n",
       "      <td>1.797722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  2.396090  2.092611  2.073392  1.988262  1.953473  2.450997  1.631040   \n",
       "1  1.775596  1.829438  2.054768  1.577190  1.594549  1.373357  1.946647   \n",
       "2  1.835679  1.612100  2.174908  2.084460  2.472896  2.029110  2.410107   \n",
       "3  2.420241  2.158485  1.958602  1.903787  2.230522  1.984789  1.964441   \n",
       "4  1.646926  1.778067  1.760959  1.894087  1.888225  2.228021  2.489542   \n",
       "\n",
       "         7         8         9   ...        15        16        17        18  \\\n",
       "0  1.746182  1.898050  2.380148  ...  1.703454  2.502966  2.119108  2.106098   \n",
       "1  1.841420  1.595761  2.538094  ...  1.974274  1.621608  2.003085  2.076871   \n",
       "2  2.282164  2.208201  2.106240  ...  2.035652  2.065291  2.197711  2.288806   \n",
       "3  2.360795  1.820773  2.116560  ...  2.040977  1.511381  1.834332  2.070046   \n",
       "4  2.326377  1.969615  2.001316  ...  2.063858  2.341009  1.844115  2.076399   \n",
       "\n",
       "         19        20        21        22        23        24  \n",
       "0  2.165173  2.340826  2.170109  1.749139  1.678661  1.829647  \n",
       "1  1.788868  2.062829  2.084499  2.267568  1.536939  2.132725  \n",
       "2  2.480274  1.946207  1.947120  1.754344  2.265033  2.119050  \n",
       "3  1.911699  1.816916  2.213950  2.099758  2.259999  2.039066  \n",
       "4  1.742857  1.969530  1.821128  1.946249  1.678283  1.797722  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils_od import count_stat, confusion_matrix, confusion_matrix_threshold,plot_data ,descriptive_stat_threshold \n",
    "from pyod.utils.data import generate_data\n",
    "\n",
    "contamination = 0.05 # percentage of outliers\n",
    "n_train = 500       # number of training points\n",
    "n_test = 500        # number of testing points\n",
    "n_features = 25      # number of features\n",
    "X_train, X_test, y_train, y_test = generate_data(\n",
    "    n_train=n_train, \n",
    "    n_test=n_test, \n",
    "    n_features= n_features, \n",
    "    contamination=contamination, \n",
    "    random_state=123)\n",
    "\n",
    "X_train_pd = pd.DataFrame(X_train)\n",
    "X_train_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05bcc7a0-bafc-425e-af0a-e48a584cbf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x240 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(5, 3), dpi=80)\n",
    "plt.scatter(X_train_pd[0], X_train_pd[1], c=y_train, alpha=0.8)\n",
    "plt.title('Scatter plot')\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8453e735-4c51-4ab3-93e0-a1b83fd15a42",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea5b38f5-3d40-43b2-a465-1810e5a0b363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 25)                650       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 25)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 25)                650       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 25)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 52        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 6         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 25)                75        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,433\n",
      "Trainable params: 1,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 15:59:52.663979: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 6ms/step - loss: 3.1595 - val_loss: 1.9899\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.7486 - val_loss: 1.8215\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.5028 - val_loss: 1.7136\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.3623 - val_loss: 1.6374\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.2597 - val_loss: 1.5804\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.1817 - val_loss: 1.5403\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.1291 - val_loss: 1.5053\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.0834 - val_loss: 1.4731\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.0406 - val_loss: 1.4417\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.0041 - val_loss: 1.4108\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.9665 - val_loss: 1.3811\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.9283 - val_loss: 1.3526\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.8977 - val_loss: 1.3248\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.8625 - val_loss: 1.2955\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.8209 - val_loss: 1.2635\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.7956 - val_loss: 1.2353\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.7642 - val_loss: 1.2084\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.7344 - val_loss: 1.1817\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.7008 - val_loss: 1.1566\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.6711 - val_loss: 1.1323\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.6610 - val_loss: 1.1100\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.6326 - val_loss: 1.0881\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.6051 - val_loss: 1.0666\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.5919 - val_loss: 1.0455\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.5538 - val_loss: 1.0272\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.5307 - val_loss: 1.0101\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.5264 - val_loss: 0.9934\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4985 - val_loss: 0.9777\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.4867 - val_loss: 0.9624\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4747 - val_loss: 0.9496\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4639 - val_loss: 0.9367\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4391 - val_loss: 0.9247\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4334 - val_loss: 0.9139\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4189 - val_loss: 0.9033\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4278 - val_loss: 0.8939\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3983 - val_loss: 0.8852\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3945 - val_loss: 0.8764\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3671 - val_loss: 0.8682\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3722 - val_loss: 0.8606\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3546 - val_loss: 0.8536\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3583 - val_loss: 0.8466\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3513 - val_loss: 0.8402\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3554 - val_loss: 0.8341\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3391 - val_loss: 0.8277\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3295 - val_loss: 0.8222\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3165 - val_loss: 0.8162\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3239 - val_loss: 0.8111\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3044 - val_loss: 0.8058\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2882 - val_loss: 0.8014\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.2915 - val_loss: 0.7969\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3004 - val_loss: 0.7927\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2849 - val_loss: 0.7884\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2811 - val_loss: 0.7845\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2626 - val_loss: 0.7805\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2813 - val_loss: 0.7771\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2581 - val_loss: 0.7734\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2657 - val_loss: 0.7704\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2604 - val_loss: 0.7673\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2610 - val_loss: 0.7647\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2590 - val_loss: 0.7615\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2500 - val_loss: 0.7586\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2486 - val_loss: 0.7563\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2353 - val_loss: 0.7534\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2331 - val_loss: 0.7493\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2333 - val_loss: 0.7467\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2281 - val_loss: 0.7447\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2300 - val_loss: 0.7428\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2317 - val_loss: 0.7398\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2209 - val_loss: 0.7373\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2189 - val_loss: 0.7357\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2144 - val_loss: 0.7342\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2100 - val_loss: 0.7324\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2002 - val_loss: 0.7297\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2199 - val_loss: 0.7280\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1995 - val_loss: 0.7255\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1982 - val_loss: 0.7234\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2041 - val_loss: 0.7216\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1978 - val_loss: 0.7202\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1952 - val_loss: 0.7187\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1998 - val_loss: 0.7170\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1880 - val_loss: 0.7149\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1927 - val_loss: 0.7139\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1867 - val_loss: 0.7124\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1963 - val_loss: 0.7112\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1950 - val_loss: 0.7093\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1853 - val_loss: 0.7083\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1868 - val_loss: 0.7072\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1815 - val_loss: 0.7058\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1727 - val_loss: 0.7045\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1866 - val_loss: 0.7031\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1748 - val_loss: 0.7017\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1758 - val_loss: 0.6998\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1839 - val_loss: 0.6986\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1807 - val_loss: 0.6972\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1718 - val_loss: 0.6970\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1740 - val_loss: 0.6949\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1755 - val_loss: 0.6940\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1699 - val_loss: 0.6927\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1765 - val_loss: 0.6924\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1761 - val_loss: 0.6911\n",
      "16/16 [==============================] - 0s 288us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoEncoder(batch_size=32, contamination=0.05, dropout_rate=0.2, epochs=100,\n",
       "      hidden_activation='relu', hidden_neurons=[2, 2], l2_regularizer=0.1,\n",
       "      loss=<function mean_squared_error at 0x29298ef70>, optimizer='adam',\n",
       "      output_activation='sigmoid', preprocessing=True, random_state=None,\n",
       "      validation_size=0.1, verbose=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "\n",
    "#Â 1 input layer with as many neurons as features in the input\n",
    "# 2 Hidden Layers with 2 neurons each.\n",
    "# 1 output layer with as many neurons as features in the desired output\n",
    "atcdr = AutoEncoder(contamination=0.05, hidden_neurons =[2, 2])\n",
    "atcdr.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ec6fa50-0f8b-4ef3-bb1f-581aa69158bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 293us/step\n",
      "16/16 [==============================] - 0s 288us/step\n",
      "16/16 [==============================] - 0s 313us/step\n",
      "16/16 [==============================] - 0s 322us/step\n",
      "The threshold for the defined contamination rate: 4.123853537071335\n",
      "The training data: {0: 475, 1: 25}\n",
      "The training data: {0: 475, 1: 25}\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "y_train_scores = atcdr.decision_function(X_train)\n",
    "y_train_pred = atcdr.predict(X_train)\n",
    "\n",
    "# Test data\n",
    "y_test_scores = atcdr.decision_function(X_test)\n",
    "y_test_pred = atcdr.predict(X_test) # outlier labels (0 or 1)\n",
    "\n",
    "# Threshold for the defined comtanimation rate\n",
    "print(\"The threshold for the defined contamination rate:\" , atcdr.threshold_)\n",
    "print(\"The training data:\", count_stat(y_train_pred))\n",
    "print(\"The training data:\", count_stat(y_test_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bcb8ff5",
   "metadata": {},
   "source": [
    "###Â Understand parameters\n",
    "* Model: \n",
    "- - 1 input layer with as many neurons as features in the input\n",
    "- - 2 Hidden Layers with 2 neurons each.\n",
    "- - 1 output layer with as many neurons as features in the desired output\n",
    "* Dropout layer after each dense layer with a 0.2 rate. (20% of the neurons are removed to avoid overfitting)\n",
    "* In this case, there are 25 neurons in the input layer, therefore 1433 parameters to be trained.\n",
    "* batch_size: Data is divided into batchs during training. It can reduce computational burden for gradient descent ( when optimizing parameters to minimize the loss function ).\n",
    "- - Example: Assume there are 100.000 instances. In the gradient descent we may compute and sum up the 100.000 gradients to update the parameters of the model at every single update. This requires a lot of memory and computation. Hence, we will update the parameters using batches of 32 instances.\n",
    "* l2_regularization: Parameter that adds a penalty value to penalize the loss when obtaining large magnitude of weights. ( Avoiding exploding gradients )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a150d79-9337-4890-ae82-bb312d166a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'contamination': 0.05,\n",
       " 'dropout_rate': 0.2,\n",
       " 'epochs': 100,\n",
       " 'hidden_activation': 'relu',\n",
       " 'hidden_neurons': [2, 2],\n",
       " 'l2_regularizer': 0.1,\n",
       " 'loss': <function tensorflow.python.keras.losses.mean_squared_error(y_true, y_pred)>,\n",
       " 'optimizer': 'adam',\n",
       " 'output_activation': 'sigmoid',\n",
       " 'preprocessing': True,\n",
       " 'random_state': None,\n",
       " 'validation_size': 0.1,\n",
       " 'verbose': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atcdr.get_params()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1943f6f-327d-497b-9a4a-5ef69864ddc1",
   "metadata": {},
   "source": [
    "### Determine the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9015e76e-d382-4bfb-867f-96eccef1d5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x320 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 4), dpi=80)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_train_scores, bins='auto') # arguments are passed to np.histogram\n",
    "plt.title(\"Outlier score\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60413add",
   "metadata": {},
   "source": [
    "A reasonable threshold could be 4.2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33284af5-5a22-465a-ad83-78bb81e3cd9e",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82b6f4a1-7215-4874-a32a-450a70ff1af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Count</th>\n",
       "      <th>Count %</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>Anomaly_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>475</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Outlier</td>\n",
       "      <td>25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>20.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group  Count  Count %     0     1     2     3     4     5    6  ...  \\\n",
       "0   Normal    475     95.0  2.01  2.01  2.02  2.00  2.00  2.00  2.0  ...   \n",
       "1  Outlier     25      5.0 -0.02  0.21 -0.05  0.13  0.01 -0.32  0.2  ...   \n",
       "\n",
       "     16    17    18    19    20    21   22    23    24  Anomaly_Score  \n",
       "0  1.99  2.02  2.01  2.00  1.99  2.00  2.0  2.01  1.99           2.11  \n",
       "1 -0.07 -0.30  0.11  0.18  0.14 -0.28  0.2  0.09 -0.10          20.27  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = atcdr.threshold_ # Or other value from the above histogram\n",
    "\n",
    "def descriptive_stat_threshold(df,pred_score, threshold):\n",
    "    # Let's see how many '0's and '1's.\n",
    "    df = pd.DataFrame(df)\n",
    "    df['Anomaly_Score'] = pred_score\n",
    "    df['Group'] = np.where(df['Anomaly_Score']< threshold, 'Normal', 'Outlier')\n",
    "\n",
    "    # Now let's show the summary statistics:\n",
    "    cnt = df.groupby('Group')['Anomaly_Score'].count().reset_index().rename(columns={'Anomaly_Score':'Count'})\n",
    "    cnt['Count %'] = (cnt['Count'] / cnt['Count'].sum()) * 100 # The count and count %\n",
    "    stat = df.groupby('Group').mean().round(2).reset_index() # The avg.\n",
    "    stat = cnt.merge(stat, left_on='Group',right_on='Group') # Put the count and the avg. together\n",
    "    return (stat)\n",
    "\n",
    "descriptive_stat_threshold(X_train,y_train_scores, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9a42470-3753-4067-a35d-9832310d5379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Count</th>\n",
       "      <th>Count %</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>Anomaly_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>475</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Outlier</td>\n",
       "      <td>25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group  Count  Count %     0     1     2     3     4     5     6  ...  \\\n",
       "0   Normal    475     95.0  2.00  1.98  2.02  2.01  1.99  2.01  1.99  ...   \n",
       "1  Outlier     25      5.0 -0.22 -0.45  0.04 -0.26 -0.34 -0.14  0.12  ...   \n",
       "\n",
       "     16   17    18    19    20    21    22    23    24  Anomaly_Score  \n",
       "0  2.00  2.0  2.01  2.00  2.00  1.99  2.01  1.98  1.99           2.10  \n",
       "1 -0.07 -0.1  0.15  0.02 -0.36  0.12  0.50 -0.08  0.36          20.93  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_stat_threshold(X_test,y_test_scores, atcdr.threshold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de5405d9-690b-43ad-a4a1-6c32156ac9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0   1\n",
       "Actual         \n",
       "0.0     475   0\n",
       "1.0       0  25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train,y_train_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc91fcbc-aa09-4c08-b0c7-d77310ccc904",
   "metadata": {},
   "source": [
    "### Aggregate Model Predictions to Achieve Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73d2bb66-b6ef-44fc-9c9f-0dbfb4b484a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 25)                650       \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 25)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 25)                650       \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 25)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 52        \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 2)                 6         \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 25)                75        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,433\n",
      "Trainable params: 1,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4.2854 - val_loss: 2.3220\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 3.4656 - val_loss: 2.0502\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.9821 - val_loss: 1.8817\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.6858 - val_loss: 1.7746\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.4809 - val_loss: 1.6963\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.3668 - val_loss: 1.6301\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.2688 - val_loss: 1.5785\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.1980 - val_loss: 1.5330\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.1333 - val_loss: 1.4939\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.0851 - val_loss: 1.4567\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.0294 - val_loss: 1.4248\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.9902 - val_loss: 1.3915\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.9536 - val_loss: 1.3580\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.9146 - val_loss: 1.3260\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.8764 - val_loss: 1.2936\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.8447 - val_loss: 1.2630\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.8080 - val_loss: 1.2319\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.7784 - val_loss: 1.2017\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.7461 - val_loss: 1.1740\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.7132 - val_loss: 1.1474\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.6896 - val_loss: 1.1210\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.6525 - val_loss: 1.0967\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.6310 - val_loss: 1.0728\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.6154 - val_loss: 1.0526\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.6043 - val_loss: 1.0334\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.5888 - val_loss: 1.0162\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.5741 - val_loss: 0.9998\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.5515 - val_loss: 0.9843\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.5231 - val_loss: 0.9697\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.5058 - val_loss: 0.9540\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4952 - val_loss: 0.9384\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4805 - val_loss: 0.9269\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4753 - val_loss: 0.9155\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4550 - val_loss: 0.9056\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4589 - val_loss: 0.8955\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4449 - val_loss: 0.8865\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4265 - val_loss: 0.8769\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4120 - val_loss: 0.8681\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4107 - val_loss: 0.8602\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3857 - val_loss: 0.8529\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3774 - val_loss: 0.8453\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3921 - val_loss: 0.8382\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3785 - val_loss: 0.8315\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3664 - val_loss: 0.8252\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3725 - val_loss: 0.8178\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3620 - val_loss: 0.8086\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3410 - val_loss: 0.8027\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3515 - val_loss: 0.7976\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3462 - val_loss: 0.7928\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3378 - val_loss: 0.7880\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3239 - val_loss: 0.7836\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3241 - val_loss: 0.7794\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3122 - val_loss: 0.7749\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3236 - val_loss: 0.7705\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3176 - val_loss: 0.7661\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3116 - val_loss: 0.7625\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3044 - val_loss: 0.7587\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3137 - val_loss: 0.7554\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2830 - val_loss: 0.7519\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2753 - val_loss: 0.7484\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2933 - val_loss: 0.7452\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2750 - val_loss: 0.7421\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2821 - val_loss: 0.7391\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2823 - val_loss: 0.7361\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2891 - val_loss: 0.7332\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2645 - val_loss: 0.7307\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2749 - val_loss: 0.7282\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2566 - val_loss: 0.7256\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2602 - val_loss: 0.7231\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2466 - val_loss: 0.7209\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2675 - val_loss: 0.7184\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2476 - val_loss: 0.7155\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2374 - val_loss: 0.7130\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2522 - val_loss: 0.7108\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2411 - val_loss: 0.7086\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2307 - val_loss: 0.7069\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2326 - val_loss: 0.7053\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2363 - val_loss: 0.7036\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2323 - val_loss: 0.7016\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2345 - val_loss: 0.6999\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2288 - val_loss: 0.6981\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2328 - val_loss: 0.6964\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2145 - val_loss: 0.6946\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2191 - val_loss: 0.6928\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2189 - val_loss: 0.6909\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2133 - val_loss: 0.6889\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2241 - val_loss: 0.6877\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2171 - val_loss: 0.6860\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2215 - val_loss: 0.6843\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2123 - val_loss: 0.6828\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2170 - val_loss: 0.6814\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2120 - val_loss: 0.6797\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2011 - val_loss: 0.6780\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1941 - val_loss: 0.6765\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2122 - val_loss: 0.6755\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2023 - val_loss: 0.6740\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1970 - val_loss: 0.6727\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1877 - val_loss: 0.6716\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1972 - val_loss: 0.6700\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1996 - val_loss: 0.6688\n",
      "16/16 [==============================] - 0s 352us/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 25)                650       \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 25)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 25)                650       \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 25)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 10)                260       \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 2)                 22        \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                30        \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 25)                275       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,887\n",
      "Trainable params: 1,887\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4.6167 - val_loss: 2.8204\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 3.9158 - val_loss: 2.5315\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 3.4630 - val_loss: 2.3418\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 3.1341 - val_loss: 2.2165\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.9316 - val_loss: 2.0951\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.7224 - val_loss: 1.9909\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.5808 - val_loss: 1.9065\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.4895 - val_loss: 1.8268\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.3577 - val_loss: 1.7565\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.2415 - val_loss: 1.6774\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.1506 - val_loss: 1.6101\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.0706 - val_loss: 1.5572\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.0013 - val_loss: 1.5040\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.9398 - val_loss: 1.4560\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.8779 - val_loss: 1.4055\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.8265 - val_loss: 1.3568\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.7808 - val_loss: 1.3125\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.7258 - val_loss: 1.2719\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.6857 - val_loss: 1.2333\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.6378 - val_loss: 1.1993\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.5945 - val_loss: 1.1677\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.5651 - val_loss: 1.1406\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.5367 - val_loss: 1.1156\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.5082 - val_loss: 1.0937\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4904 - val_loss: 1.0741\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4749 - val_loss: 1.0576\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4545 - val_loss: 1.0421\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4337 - val_loss: 1.0284\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4170 - val_loss: 1.0133\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3969 - val_loss: 1.0001\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3880 - val_loss: 0.9888\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3717 - val_loss: 0.9795\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3585 - val_loss: 0.9687\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3488 - val_loss: 0.9537\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3458 - val_loss: 0.9448\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3333 - val_loss: 0.9375\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3210 - val_loss: 0.9307\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3135 - val_loss: 0.9244\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3155 - val_loss: 0.9185\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3008 - val_loss: 0.9131\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2955 - val_loss: 0.9077\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2923 - val_loss: 0.9027\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2728 - val_loss: 0.8974\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2758 - val_loss: 0.8926\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2745 - val_loss: 0.8888\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2596 - val_loss: 0.8844\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2670 - val_loss: 0.8793\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2536 - val_loss: 0.8753\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2454 - val_loss: 0.8719\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2501 - val_loss: 0.8687\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2460 - val_loss: 0.8650\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2357 - val_loss: 0.8613\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2343 - val_loss: 0.8573\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2335 - val_loss: 0.8544\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2242 - val_loss: 0.8507\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2281 - val_loss: 0.8478\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2250 - val_loss: 0.8447\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2136 - val_loss: 0.8427\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2155 - val_loss: 0.8443\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2225 - val_loss: 0.8394\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2074 - val_loss: 0.8354\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2121 - val_loss: 0.8318\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2046 - val_loss: 0.8290\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1970 - val_loss: 0.8269\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2010 - val_loss: 0.8252\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1945 - val_loss: 0.8232\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1920 - val_loss: 0.8210\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1938 - val_loss: 0.8188\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1924 - val_loss: 0.8168\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.1914 - val_loss: 0.8148\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1875 - val_loss: 0.8134\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1841 - val_loss: 0.8133\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1817 - val_loss: 0.8099\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1772 - val_loss: 0.8071\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1732 - val_loss: 0.8049\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1757 - val_loss: 0.8037\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1748 - val_loss: 0.8017\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1696 - val_loss: 0.7984\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1702 - val_loss: 0.7962\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1643 - val_loss: 0.7949\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1735 - val_loss: 0.7937\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1636 - val_loss: 0.7917\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1615 - val_loss: 0.7902\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1609 - val_loss: 0.7892\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1610 - val_loss: 0.7873\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1537 - val_loss: 0.7864\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1616 - val_loss: 0.7865\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1526 - val_loss: 0.7837\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1548 - val_loss: 0.7831\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1450 - val_loss: 0.7820\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1435 - val_loss: 0.7794\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1523 - val_loss: 0.7797\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1480 - val_loss: 0.7788\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1418 - val_loss: 0.7746\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1461 - val_loss: 0.7728\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1443 - val_loss: 0.7723\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1410 - val_loss: 0.7713\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1405 - val_loss: 0.7698\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1370 - val_loss: 0.7695\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1402 - val_loss: 0.7692\n",
      "16/16 [==============================] - 0s 343us/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 25)                650       \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 25)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 25)                650       \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 25)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 15)                390       \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 15)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 10)                160       \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 2)                 22        \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                30        \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 15)                165       \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 15)                0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 25)                400       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,467\n",
      "Trainable params: 2,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 3.1215 - val_loss: 1.2718\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.7374 - val_loss: 1.1996\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.5319 - val_loss: 1.1440\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.3831 - val_loss: 1.0908\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.2773 - val_loss: 1.0355\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.1741 - val_loss: 0.9767\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.0814 - val_loss: 0.9121\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.9883 - val_loss: 0.8478\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.8960 - val_loss: 0.7885\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.8228 - val_loss: 0.7413\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.7567 - val_loss: 0.7016\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.7053 - val_loss: 0.6681\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.6718 - val_loss: 0.6386\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.6297 - val_loss: 0.6129\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.6001 - val_loss: 0.5899\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.5688 - val_loss: 0.5680\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.5378 - val_loss: 0.5508\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.5180 - val_loss: 0.5347\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.5051 - val_loss: 0.5189\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4793 - val_loss: 0.5039\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.4632 - val_loss: 0.4907\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.4489 - val_loss: 0.4792\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.4370 - val_loss: 0.4677\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.4196 - val_loss: 0.4569\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.4092 - val_loss: 0.4473\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.3872 - val_loss: 0.4384\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.3881 - val_loss: 0.4308\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3703 - val_loss: 0.4225\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3633 - val_loss: 0.4153\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3585 - val_loss: 0.4116\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3409 - val_loss: 0.4047\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3351 - val_loss: 0.3972\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3270 - val_loss: 0.3905\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3172 - val_loss: 0.3850\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.3224 - val_loss: 0.3793\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.3112 - val_loss: 0.3743\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.3025 - val_loss: 0.3696\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.3015 - val_loss: 0.3678\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2946 - val_loss: 0.3636\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2909 - val_loss: 0.3591\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2839 - val_loss: 0.3551\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2761 - val_loss: 0.3529\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2796 - val_loss: 0.3484\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2701 - val_loss: 0.3439\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2690 - val_loss: 0.3405\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2607 - val_loss: 0.3377\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2599 - val_loss: 0.3348\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.2545 - val_loss: 0.3320\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2524 - val_loss: 0.3314\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2464 - val_loss: 0.3284\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2460 - val_loss: 0.3247\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2414 - val_loss: 0.3217\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2386 - val_loss: 0.3210\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2402 - val_loss: 0.3185\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2369 - val_loss: 0.3160\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2333 - val_loss: 0.3139\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2276 - val_loss: 0.3114\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2291 - val_loss: 0.3092\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2260 - val_loss: 0.3077\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2238 - val_loss: 0.3056\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2245 - val_loss: 0.3038\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2192 - val_loss: 0.3027\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2151 - val_loss: 0.3017\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2130 - val_loss: 0.2996\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.2179 - val_loss: 0.2983\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2133 - val_loss: 0.2970\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.2119 - val_loss: 0.2959\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2085 - val_loss: 0.2942\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2075 - val_loss: 0.2926\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.2055 - val_loss: 0.2913\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2018 - val_loss: 0.2904\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.2047 - val_loss: 0.2895\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.2015 - val_loss: 0.2878\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.2036 - val_loss: 0.2870\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1975 - val_loss: 0.2880\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1.1976 - val_loss: 0.2864\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1962 - val_loss: 0.2845\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1929 - val_loss: 0.2830\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1922 - val_loss: 0.2821\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1921 - val_loss: 0.2824\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1898 - val_loss: 0.2815\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1927 - val_loss: 0.2815\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1883 - val_loss: 0.2802\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1887 - val_loss: 0.2788\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1848 - val_loss: 0.2773\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1826 - val_loss: 0.2762\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1848 - val_loss: 0.2752\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1827 - val_loss: 0.2744\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1821 - val_loss: 0.2734\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1791 - val_loss: 0.2726\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1795 - val_loss: 0.2717\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1782 - val_loss: 0.2712\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1798 - val_loss: 0.2705\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1781 - val_loss: 0.2697\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1810 - val_loss: 0.2690\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1753 - val_loss: 0.2682\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1767 - val_loss: 0.2676\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1775 - val_loss: 0.2668\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1747 - val_loss: 0.2664\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 1.1756 - val_loss: 0.2661\n",
      "16/16 [==============================] - 0s 331us/step\n",
      "16/16 [==============================] - 0s 314us/step\n",
      "16/16 [==============================] - 0s 309us/step\n",
      "16/16 [==============================] - 0s 347us/step\n",
      "16/16 [==============================] - 0s 291us/step\n",
      "16/16 [==============================] - 0s 329us/step\n",
      "16/16 [==============================] - 0s 313us/step\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.combination import aom, moa, average, maximization\n",
    "from pyod.utils.utility import standardizer\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "atcdr1 = AutoEncoder(contamination=0.05, hidden_neurons =[2, 2])\n",
    "atcdr2 = AutoEncoder(contamination=0.05, hidden_neurons =[10, 2, 10])\n",
    "atcdr3 = AutoEncoder(contamination=0.05, hidden_neurons =[15, 10, 2, 10, 15] )\n",
    "\n",
    "# Standardize data\n",
    "X_train_norm, X_test_norm = standardizer(X_train, X_test)\n",
    "\n",
    "# Just prepare data frames so we can store the model results. There are three models.\n",
    "train_scores = np.zeros([X_train.shape[0], 3])\n",
    "test_scores = np.zeros([X_test.shape[0], 3])\n",
    "atcdr1.fit(X_train_norm)\n",
    "atcdr2.fit(X_train_norm)\n",
    "atcdr3.fit(X_train_norm)\n",
    "    \n",
    "# Store the results in each column:\n",
    "train_scores[:, 0] = atcdr1.decision_function(X_train_norm) \n",
    "train_scores[:, 1] = atcdr2.decision_function(X_train_norm) \n",
    "train_scores[:, 2] = atcdr3.decision_function(X_train_norm)\n",
    "test_scores[:, 0] = atcdr1.decision_function(X_test_norm) \n",
    "test_scores[:, 1] = atcdr2.decision_function(X_test_norm) \n",
    "test_scores[:, 2] = atcdr3.decision_function(X_test_norm)\n",
    "\n",
    "# Decision scores have to be normalized before combination\n",
    "train_scores_norm, test_scores_norm = standardizer(train_scores,test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ec9923e-c4df-432e-96dd-e92e5b66497c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x320 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 4), dpi=80)\n",
    "# Combination by average\n",
    "y_train_by_average = average(train_scores_norm)\n",
    "y_test_by_average = average(test_scores_norm)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_train_by_average, bins='auto') # arguments are passed to np.histogram\n",
    "plt.title(\"Combination by average\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37c2b2e2-ebe3-490c-8008-c9984613a60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Count</th>\n",
       "      <th>Count %</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>Anomaly_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>475</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Outlier</td>\n",
       "      <td>25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>4.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group  Count  Count %     0     1     2     3     4     5    6  ...  \\\n",
       "0   Normal    475     95.0  2.01  2.01  2.02  2.00  2.00  2.00  2.0  ...   \n",
       "1  Outlier     25      5.0 -0.02  0.21 -0.05  0.13  0.01 -0.32  0.2  ...   \n",
       "\n",
       "     16    17    18    19    20    21   22    23    24  Anomaly_Score  \n",
       "0  1.99  2.02  2.01  2.00  1.99  2.00  2.0  2.01  1.99          -0.23  \n",
       "1 -0.07 -0.30  0.11  0.18  0.14 -0.28  0.2  0.09 -0.10           4.32  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_stat_threshold(X_train,y_train_by_average, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6a23053-4d1a-4d3e-992a-416439aa5ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Count</th>\n",
       "      <th>Count %</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>Anomaly_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>475</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Outlier</td>\n",
       "      <td>25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.36</td>\n",
       "      <td>4.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group  Count  Count %     0     1     2     3     4     5     6  ...  \\\n",
       "0   Normal    475     95.0  2.00  1.98  2.02  2.01  1.99  2.01  1.99  ...   \n",
       "1  Outlier     25      5.0 -0.22 -0.45  0.04 -0.26 -0.34 -0.14  0.12  ...   \n",
       "\n",
       "     16   17    18    19    20    21    22    23    24  Anomaly_Score  \n",
       "0  2.00  2.0  2.01  2.00  2.00  1.99  2.01  1.98  1.99          -0.23  \n",
       "1 -0.07 -0.1  0.15  0.02 -0.36  0.12  0.50 -0.08  0.36           4.48  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_stat_threshold(X_test,y_test_by_average, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19589ca4-8802-4014-9d14-3c378f31c39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0   1\n",
       "Actual         \n",
       "0.0     475   0\n",
       "1.0       0  25"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix_threshold(y_train,y_train_by_average,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31f86490-1377-4b2c-8fad-21c622a33e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0   1\n",
       "Actual         \n",
       "0.0     475   0\n",
       "1.0       0  25"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix_threshold(y_test,y_test_by_average, 0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de964e8f",
   "metadata": {},
   "source": [
    "## Summary Autoencoders\n",
    "* \n",
    "*\n",
    "*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
